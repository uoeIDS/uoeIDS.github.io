---
title: "Model checking and multiple linear regression"
subtitle: "<br><br> Introduction to Data Science"
author: "University of Edinburgh"
date: "<br> 2023/2024"
output:
  xaringan::moon_reader:
    css: ["./xaringan-themer.css", "./slides.css"]
    lib_dir: libs
    anchor_sections: FALSE
    nature:
      ratio: "16:9"
      highlightLines: true
      highlightStyle: solarized-light
      countIncrementalSlides: false

---


```{r packages, echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(magick)
library(dplyr)
library(tidymodels)
library(ggtext)
library(knitr)
library(kableExtra)
library(xaringanExtra)
library(Tmisc)
# library(emo)
library(patchwork)
library(plotly)
library(widgetframe)

xaringanExtra::use_panelset()
```


```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 6,
  tibble.width = 65,
  width = 65
  )
# figure height, width, dpi
knitr::opts_chunk$set(echo = TRUE, 
                      fig.width = 8, 
                      fig.asp = 0.618,
                      out.width = "60%",
                      fig.align = "center",
                      dpi = 300,
                      message = FALSE)
# ggplot2
ggplot2::theme_set(ggplot2::theme_gray(base_size = 16))
# set seed
set.seed(1234)
# fontawesome
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# output number of lines
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

```

layout: true
  
<div class="my-footer">
<span>
University of Edinburgh
</span>
</div> 

---
## Topics

- Assessing model fit.
- Linearity and transformations.
- Multiple linear regression
- Interaction effects
- Model comparison



---
class: middle

# Model checking

---

## "Linear" models

- We're fitting a "linear" model, which assumes a linear relationship between our explanatory and response variables.
- But how do we assess this?

---

## Data: Paris Paintings

```{r message=FALSE}
pp <- read_csv("data/paris-paintings.csv", na = c("n/a", "", "NA"))
```

- Number of observations: `r nrow(pp)`
- Number of variables: `r ncol(pp)`

---

## Graphical diagnostic: residuals plot

.panelset[
.panel[.panel-name[Plot]
```{r ref.label = "residuals-plot", echo = FALSE, warning = FALSE, out.width = "60%"}
```
]
.panel[.panel-name[Code]
```{r residuals-plot, fig.show="hide"}
ht_wt_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(Height_in ~ Width_in, data = pp)

ht_wt_fit_aug <- augment(ht_wt_fit$fit) #<<

ggplot(ht_wt_fit_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted height", y = "Residuals")
```
]

.panel[.panel-name[Augment]
```{r}
ht_wt_fit_aug
```
]
]

---

## Looking for...

- Residuals distributed randomly around 0
- With no visible pattern along the x or y axes

```{r out.width = "60%", echo=FALSE}
df <- tibble(
  fake_resid = rnorm(1000, mean = 0, sd = 30),
  fake_predicted = runif(1000, min = 0, max = 200)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

.large[
**Fan shapes**
]

```{r out.width = "60%", echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_resid = c(rnorm(100, mean = 0, sd = 1), 
                 rnorm(100, mean = 0, sd = 15), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 20), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 50), 
                 rnorm(100, mean = 0, sd = 35), 
                 rnorm(100, mean = 0, sd = 40),
                 rnorm(200, mean = 0, sd = 80)),
  fake_predicted = seq(0.2, 200, 0.2)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

.large[
**Groups of patterns**
]

```{r out.width = "60%", echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = c(
    rnorm(500, mean = -20, sd = 10),
    rnorm(500, mean = 10, sd = 10)
  )
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

.large[
**Residuals correlated with predicted values**
]

```{r out.width = "60%", echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = fake_predicted + rnorm(1000, mean = 0, sd = 50)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted", y = "Residuals")
```

---

## Not looking for...

.large[
**Any patterns!**
]

```{r out.width = "60%", echo=FALSE}
set.seed(12346)
df <- tibble(
  fake_predicted = seq(-100, 100, 0.4),
  fake_resid = -5*fake_predicted^2 - 3*fake_predicted + 20000 + rnorm(501, mean = 0, sd = 10000)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted", y = "Residuals")
```

---

.question[
What patterns does the residuals plot reveal that should make us question whether a linear model is a good fit for modeling the relationship between height and width of paintings?
]

```{r out.width = "60%", echo=FALSE}
ggplot(ht_wt_fit_aug, mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "gray", lty = "dashed") +
  labs(x = "Predicted height", y = "Residuals")
```

---

class: middle

# Linearity and transformations

---

## Data: Paris Paintings

```{r echo=FALSE, out.width = "70%"}
ggplot(data = pp, aes(x = price)) +
  geom_histogram(binwidth = 1000) +
  labs(title = "Prices of paintings")
```

---

## Price vs. width

```{r echo=FALSE, out.width = "70%", warning = FALSE}
ggplot(data = pp, aes(x = Width_in, y = price)) +
  geom_point(alpha = 0.5) +
  labs(x = "Width (in)", y = "Price (livres)")
```

---

## Focus on paintings with `Width_in < 100`

That is, paintings with width < 2.54 m

```{r}
pp_wt_lt_100 <- pp %>% 
  filter(Width_in < 100)
```

---

## Price vs. width

.question[
Which plot shows a more linear relationship?
]

.small[
  
.pull-left[
```{r message=FALSE, echo=FALSE, out.width = "100%"}
ggplot(data = pp_wt_lt_100, 
       mapping = aes(x = Width_in, y = price)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Price vs. width", 
    subtitle = "For width < 100 in",
    x = "Width (inches)", 
    y = "Price (livres)"
    )
```
]

.pull-right[
```{r message=FALSE, echo=FALSE, out.width = "100%"}
ggplot(data = pp_wt_lt_100, 
       mapping = aes(x = Width_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Log(price) vs. width", 
    subtitle = "For width < 100 in",
    x = "Width (inches)", 
    y = "Log(price) (log livres)"
    )
```
]

]

---

## Price vs. width, residuals

.question[
Which plot shows a residuals that are uncorrelated with predicted values from the model? Also, what is the unit of the residuals?
]
  
.pull-left[
```{r message=FALSE, echo=FALSE, out.width = "100%"}
price_wt_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(price ~ Width_in, data = pp_wt_lt_100)
price_wt_fit_aug <- augment(price_wt_fit$fit)

ggplot(data = price_wt_fit_aug, 
       mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Price vs. width, residuals", 
    subtitle = "For width < 100 in",
    x = "Predicted price (livres)", 
    y = "Residuals"
    )
```
]
.pull-right[
```{r message=FALSE, echo=FALSE, out.width = "100%"}
pp_wt_lt_100 <- pp_wt_lt_100 %>% 
  mutate(log_price = log(price))

lprice_wt_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Width_in, data = pp_wt_lt_100)
lprice_wt_fit_aug <- augment(lprice_wt_fit$fit)

ggplot(data = lprice_wt_fit_aug, 
       mapping = aes(x = .fitted, y = .resid)) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Log(Price) vs. width, residuals", 
    subtitle = "For width < 100 in",
    x = "Predicted log(price) (log livres)", 
    y = "Residuals"
    )
```
]

---

## Transforming the data

- We saw that `price` has a right-skewed distribution, and the relationship between price and width of painting is non-linear.

--
- In these situations a transformation applied to the response variable may be useful.

--
- In order to decide which transformation to use, we should examine the distribution of the response variable.

--
- The extremely right skewed distribution suggests that a log transformation may 
be useful.
    - log = natural log, $ln$
    - Default base of the `log` function in R is the natural log: <br>
    `log(x, base = exp(1))`
    
---

## Logged price vs. width

.question[
How do we interpret the slope of this model?
]

```{r echo=FALSE, message=FALSE, out.width="60%"}
ggplot(data = pp_wt_lt_100, mapping = aes(x = Width_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", color = "#8E2C90", se = FALSE) +
  labs(x = "Width (in)", y = "Log(price) (log livres)")
```

---

## Models with log transformation

```{r}
linear_reg() %>%
  set_engine("lm") %>%
  fit(log(price) ~ Width_in, data = pp_wt_lt_100) %>%
  tidy()
```

---


## Interpreting the slope

$$ \widehat{log(price)} = 4.67 + 0.0192 \times Width $$

- For each additional inch the painting is wider, the log price of the painting is expected to be higher, on average, by 0.0192 log livres.

--
- which is not a very useful statement...

Instead:

- For each additional inch the painting is wider, the price of the painting is expected to be higher, on average, *by a (multiplicative) factor of 1.0192* (1.0192 = exp(0.0192)).


--

In other words:

- For each additional inch the painting is wider, the price of the painting is expected to be higher, on average, *by 1.92%* (1.92 = 100 * 0.0192).

---

## Transform, or learn more?

- Data transformations may also be useful when the relationship is non-linear
- However in those cases a polynomial regression may be more appropriate
  + This is beyond the scope of this course, but you’re welcomed to try it for your final project, and I’d be happy to provide further guidance

---
class: middle

# The linear model with multiple predictors

### Two numerical predictors

---

## The data

```{r load-pp, message=FALSE}
pp <- read_csv("data/paris-paintings.csv",na = c("n/a", "", "NA")) %>%
  mutate(log_price = log(price))
```



---

## Multiple predictors

- Response variable: `log_price` 
- Explanatory variables: Width and height

.pull-left[
```{r logprice-vs-width,echo=FALSE, warning=FALSE,out.width="90%"}
ggplot(data = pp, 
       mapping = aes(x = Width_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Log(price) vs. width", 
    x = "Width (inches)", 
    y = "Log(price) (log livres)"
    )
```
]
.pull-right[
```{r logprice-vs-height,echo=FALSE, warning=FALSE,out.width="90%"}
ggplot(data = pp, 
       mapping = aes(x = Height_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Log(price) vs. height", 
    x = "Height (inches)", 
    y = "Log(price) (log livres)"
    )
```

]

---

##  Linear model with multiple predictors

```{r model-price-width-height}
pp_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Width_in + Height_in, data = pp)
tidy(pp_fit)
```

$$\widehat{log\_price} = 4.77 + 0.0269 \times width - 0.0133 \times height$$

---

##  Linear model with multiple predictors

.small[
```{r model-price-width-height2}
tidy(pp_fit) %>% mutate(exp_estimate = exp(estimate)) %>% 
  select(term, estimate, exp_estimate)
```

$$\widehat{log\_price} = 4.77 + 0.0269 \times width - 0.0133 \times height$$
]



--
- **Slope - width:** *All else held constant*, for each additional inch the painting is wider, we would expect the log-price to be higher, on average, by 0.0269.

--
- **Slope - height:** *All else held constant*, for each additional inch the painting is higher, we would expect the log-price to be lower, on average, by 0.0133.

--
- **Intercept:** The log-price of paintings that are 0 inches wide and 0 inches high is expected to be 4.77, on average. (Doesn't make sense in context.)


---

##  Linear model with multiple predictors

.small[
```{r model-price-width-height4}
tidy(pp_fit) %>% mutate(exp_estimate = exp(estimate)) %>% 
  select(term, estimate, exp_estimate)
```

$$\widehat{log\_price} = 4.77 + 0.0269 \times width - 0.0133 \times height$$
]


.question[
Interpret the slope for width in terms of price (not log-price).
]


---

##  Linear model with multiple predictors

.small[
```{r model-price-width-height3}
tidy(pp_fit) %>% mutate(exp_estimate = exp(estimate)) %>% 
  select(term, estimate, exp_estimate)
```

$$\widehat{log\_price} = 4.77 + 0.0269 \times width - 0.0133 \times height$$
]


- **Slope - width:** *All else held constant*, for each additional inch the painting is wider, we would expect the *price* to be higher, on average, *by a factor of 1.03*.
- **Slope - height:** *All else held constant*, for each additional inch the painting is higher, we would expect the *price* to be lower, on average, *by a factor of 0.98*.
- **Intercept:** The *price* of paintings that are 0 inches wide and 0 inches high is expected to be *118*, on average. (Doesn't make sense in context.)


---

## Visualizing models with multiple predictors

.panelset[
.panel[.panel-name[Plot]
.pull-left-wide[
```{r plotly-plot, echo = FALSE, fig.align="center", warning=FALSE}
p <- plot_ly(pp,
  x = ~Width_in, y = ~Height_in, z = ~log_price,
  marker = list(size = 3, color = "lightgray", alpha = 0.5, 
                line = list(color = "gray", width = 2))) %>%
  add_markers() %>%
  plotly::layout(scene = list(
    xaxis = list(title = "Width (in)"),
    yaxis = list(title = "Height (in)"),
    zaxis = list(title = "log_price")
  )) %>%
  config(displayModeBar = FALSE)
frameWidget(p)
```
]
]
.panel[.panel-name[Code]
```{r plotly-code, eval=FALSE}
p <- plot_ly(pp,
  x = ~Width_in, y = ~Height_in, z = ~log_price,
  marker = list(size = 3, color = "lightgray", alpha = 0.5, 
                line = list(color = "gray", width = 2))) %>%
  add_markers() %>%
  plotly::layout(scene = list(
    xaxis = list(title = "Width (in)"),
    yaxis = list(title = "Height (in)"),
    zaxis = list(title = "log_price")
  )) %>%
  config(displayModeBar = FALSE)
frameWidget(p)
```
]
]

---
class: middle

# The linear model with multiple predictors

### Numerical and categorical predictors

---
  
## Data: Book weight and volume
  
The `allbacks` data frame gives measurements on the volume and weight of 15 books, some of which are paperback and some of which are hardback

.pull-left[
- Volume - cubic centimetres
- Area - square centimetres
- Weight - grams
]
.pull-right[
.small[
```{r echo=FALSE}
library(DAAG)
as_tibble(allbacks) %>%
  print(n = 15)
```
]
]

.footnote[
.small[
These books are from the bookshelf of J. H. Maindonald at Australian National University.
]
]

---
  
## Book weight vs. volume
  
.pull-left[
```{r}
linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume, 
      data = allbacks) %>%
  tidy()
```
]
.pull-right[
```{r out.width = "75%", echo = FALSE, fig.align = "right"}
ggplot(allbacks, aes(x = volume, y = weight)) +
  geom_point(alpha = 0.7, size = 3)+
  geom_smooth(method="lm",se=FALSE)
```
]

---

## Book weight vs. volume and cover

.pull-left[
```{r}
allbacks %>% 
  group_by(cover) %>%
  summarise(mean_weight = mean(weight))
```

]
.pull-right[
```{r out.width = "75%", echo = FALSE, fig.align = "right"}
ggplot() +
  geom_point(allbacks, 
             mapping = aes(x = volume, y = weight, color = cover), size = 3) +
  scale_color_manual(values = c("#E48957", "#071381"))
```
]


---

## Book weight vs. volume and cover

.pull-left[
```{r}
linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, 
      data = allbacks) %>%
  tidy()
```
]
.pull-right[
```{r out.width = "75%", echo = FALSE, fig.align = "right"}
book_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks)
book_main_fit_aug <- augment(book_main_fit$fit)

ggplot() +
  geom_point(book_main_fit_aug, 
             mapping = aes(x = volume, y = weight, color = cover), size = 3) +
  geom_line(book_main_fit_aug, 
          mapping = aes(x = volume, y = .fitted, color = cover), linewidth = 1) +
  scale_color_manual(values = c("#E48957", "#071381"))
```
]

---

## Interpretation of estimates

```{r echo=FALSE}
linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks) %>%
  tidy()
```

- **Slope - volume:** *All else held constant*, for each additional cubic centimetre books are larger in volume, we would expect the weight to be higher, on average, by 0.718 grams.

--

.question[
What is the interpretation of the slope for cover?
]

---

## Interpretation of estimates

```{r echo=FALSE}
linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks) %>%
  tidy()
```

- **Slope - volume:** *All else held constant*, for each additional cubic centimetre books are larger in volume, we would expect the weight to be higher, on average, by 0.718 grams.

- **Slope - cover:** *All else held constant*, paperback books weigh, on average, 184 grams less than hardback books.

--

- **Intercept:** Hardback books with 0 volume are expected to weigh 198 grams, on average. (Doesn't make sense in context.)

---

## Parallel slopes

By adding the **main effect** for `cover` we obtained to parallel lines for each group.

```{r out.width = "50%", echo = FALSE}
ggplot() +
  geom_point(book_main_fit_aug, 
             mapping = aes(x = volume, y = weight, color = cover), size = 3) +
  geom_line(book_main_fit_aug, 
          mapping = aes(x = volume, y = .fitted, color = cover), linewidth = 1) +
  labs(title = "Main effects, parallel slopes", 
       subtitle = "weight-hat = volume + cover") +
  scale_color_manual(values = c("#E48957", "#071381"))
```

Can we fit a model so that the fitted lines for the two categories of `cover` are 
not parallel, 
or in other words, have a different slope?

---

## Two ways to model

- **Main effects:** Assuming relationship between volume and weight 
**does not vary** by whether the cover is hardback or paperback.
- **Interaction effects:** Assuming relationship between volume and weight 
**varies** by whether the cover is hardback or paperback.

---

## Interacting explanatory variables

- Including an interaction effect in the model allows for different slopes, i.e. 
nonparallel lines.
- This implies that the regression coefficient for an explanatory variable would 
change as another explanatory variable changes.
- This can be accomplished by adding an interaction variable: the product of two 
explanatory variables.

---

## Two ways to model

.pull-left[
- **Main effects:** Assuming relationship between volume and weight **does not vary** by whether the cover is hardback or paperback

- **Interaction effects:** Assuming relationship between volume and weight **varies** by whether the cover is hardback or paperback
]
.pull-right[
```{r pp-main-int-viz0, echo=FALSE, out.width="90%", fig.asp = 0.9}
book_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks)
book_main_fit_aug <- augment(book_main_fit$fit)

book_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume * cover, data = allbacks)
book_int_fit_aug <- augment(book_int_fit$fit)

p_main <- ggplot(book_main_fit_aug,
                 aes(y = weight, x = volume, color = cover)) +
  geom_point(size = 3) +
  geom_line(mapping = aes(y = .fitted), linewidth = 1) +
  scale_color_manual(values = c("#E48957", "#071381")) + 
  labs(title = "Main effects")

p_int <- ggplot(book_int_fit_aug,
                 aes(y = weight, x = volume, color = cover)) +
  geom_point(size = 3) +
  geom_line(mapping = aes(y = .fitted), linewidth = 1) +
  scale_color_manual(values = c("#E48957", "#071381")) + 
  labs(title = "Interaction effects")

p_main /
  p_int  + 
  plot_layout(guides = "collect") & theme(legend.position = "bottom")
```
]

In this example, the difference is small. 
Let's look at another example.

---

## Price, surface area, and living artist

- Explore the relationship between price of paintings and surface area, conditioned 
on whether or not the artist is still living
- First visualize and explore, then model

```{r echo=FALSE}
pp <- pp %>%
  mutate(artistliving = if_else(artistliving == 0, "Deceased", "Living"))
```

---

## Typical surface area

.panelset[
.panel[.panel-name[Plot]
.pull-left-narrow[
Typical surface area appears to be less than 1000 square inches (~ 80cm x 80cm). 

There are very few paintings that have surface area above 5000 square inches.
]
.pull-right-wide[
```{r ref.label = "viz-surf-artistliving", echo = FALSE, warning = FALSE, out.width="90%"}
```
]
]
.panel[.panel-name[Code]
```{r viz-surf-artistliving, fig.show = "hide"}
ggplot(data = pp, aes(x = Surface, fill = artistliving)) +
  geom_histogram(binwidth = 500) + 
  facet_grid(artistliving ~ .) +
  scale_fill_manual(values = c("#E48957", "#071381")) +
  guides(fill = FALSE) +
  labs(x = "Surface area", y = NULL) +
  geom_vline(xintercept = 1000) +
  geom_vline(xintercept = 5000, linetype = "dashed", color = "gray")
```
]
]

---

## Narrowing the scope

For simplicity let's focus on the paintings with `Surface < 5000`:
.midi[
```{r surf-lt-5000}
pp_Surf_lt_5000 <- pp %>%
  filter(Surface < 5000)
```
]

.panelset[
.panel[.panel-name[Plot]
```{r ref.label = "viz-surf-lt-5000-artistliving", echo = FALSE, warning = FALSE, out.width = "40%"}
```

]
.panel[.panel-name[Code]
```{r viz-surf-lt-5000-artistliving, fig.show = "hide"}
ggplot(data = pp_Surf_lt_5000, 
       aes(y = log_price, x = Surface, color = artistliving, shape = artistliving)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~artistliving) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  labs(color = "Artist", shape = "Artist")
```
]
]

---


## Fit model with main effects

- Response variable: `log_price`
- Explanatory variables: `Surface` area and `artistliving`

.midi[
```{r model-main-effects}
pp_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)
tidy(pp_main_fit)
```
]

--

$$\widehat{log\_price} = 4.88 + 0.000265 \times surface + 0.137 \times artistliving$$

---

## Solving the model

$$\widehat{log\_price} = 4.88 + 0.000265 \times surface + 0.137 \times artistliving$$

- Non-living artist: Plug in 0 for `artistliving`

\begin{align}
\widehat{log\_price} =& \color{#E48957}{4.88} + 0.000265 \times surface + \color{#E48957}{0.137 \times 0}\\
=& (\color{#E48957}{4.88+0}) + 0.000265 \times surface\\
=& \color{#E48957}{4.88} + 0.000265 \times surface
\end{align}

--
- Living artist: Plug in 1 for `artistliving`

\begin{align}
\widehat{log\_price} =& \color{#2AA3BB}{4.88} + 0.000265 \times surface + \color{#2AA3BB}{0.137 \times 1}\\
=& (\color{#2AA3BB}{4.88+0.137}) + 0.000265 \times surface\\
=& \color{#2AA3BB}{5.017} + 0.000265 \times surface
\end{align}

---

## Visualizing main effects

.pull-left-narrow[
- **Same slope:** Rate of change in price as the surface area increases does
not vary between paintings by living and non-living artists.
- **Different intercept:** Paintings by living artists are consistently more
expensive than paintings by non-living artists.
]
.pull-right-wide[
```{r out.width="100%", echo = FALSE}
pp_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface + artistliving, data = pp_Surf_lt_5000)
pp_main_fit_aug <- augment(pp_main_fit$fit)

pp_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)
pp_int_fit_aug <- augment(pp_int_fit$fit)

p_main_artist <- ggplot(
  pp_main_fit_aug,
  aes(y = log_price, x = Surface, color = artistliving)
) +
  geom_point(aes(shape = artistliving), alpha = 0.5) +
  scale_x_continuous(breaks = c(0, 2500, 5000)) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  geom_line(aes(y = .fitted), linewidth = 1.5) +
  labs(y = "log_price", title = "Main effects", color = "Artist", shape = "Artist")

p_int_artist <- ggplot(
  pp_int_fit_aug,
  aes(y = log_price, x = Surface, color = artistliving)
) +
  geom_point(aes(shape = artistliving), alpha = 0.5) +
  scale_x_continuous(breaks = c(0, 2500, 5000)) +
  scale_color_manual(values = c("#E48957", "#071381")) +
  geom_line(aes(y = .fitted), linewidth = 1.5) +
  labs(y = "log_price", title = "Interaction effects", color = "Artist", shape = "Artist")

p_main_artist
```
]


---

## Interaction: `Surface * artistliving`

```{r out.width="60%", echo = FALSE}
p_int_artist
```

---

## Fit model with interaction effects

- Response variable: log_price
- Explanatory variables: `Surface` area, `artistliving`, and their interaction

.midi[
```{r model-interaction-effects}
pp_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface * artistliving, data = pp_Surf_lt_5000)
tidy(pp_int_fit)
```
]

---

## Fit model with interaction effects

- Response variable: log_price
- Explanatory variables: `Surface` area, `artistliving`, and their interaction

.midi[
```{r model-interaction-effects2}
pp_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(log_price ~ Surface + artistliving + 
                  Surface * artistliving, data = pp_Surf_lt_5000)
tidy(pp_int_fit)
```
]

---

## Linear model with interaction effects

.midi[
```{r model-interaction-effects-tidy, echo=FALSE}
tidy(pp_int_fit)
```
]

.small[
\begin{align}
\widehat{log\_price} = 4.91 &+ 0.00021 \times surface - 0.126 \times artistliving + 0.00048 \times surface * artistliving
\end{align}
]

---

## Solving the model

.small[
\begin{align}
\widehat{log\_price} = 4.91 &+ 0.00021 \times surface - 0.126 \times artistliving + 0.00048 \times surface * artistliving
\end{align}
]

- Non-living artist: Plug in 0 for `artistliving`

.small[
\begin{align}
\widehat{log\_price} =& \color{#E48957}{4.91} + \color{#FFCE7E}{0.00021} \times surface \color{#E48957}{- 0.126 \times 0} + \color{#FFCE7E}{0.00048 \times 0} \times surface\\
=& (\color{#E48957}{4.91+0}) + (\color{#FFCE7E}{0.00021+0}) \times surface\\
=& \color{#E48957}{4.91} + \color{#FFCE7E}{0.00021} \times surface
\end{align}
]

--

- Living artist: Plug in 1 for `artistliving`
.small[
\begin{align}
\widehat{log\_price} =& \color{#2AA3BB}{4.91} + \color{#ABC8C0}{0.00021} \times surface \color{#2AA3BB}{- 0.126 \times 1} + \color{#ABC8C0}{0.00048 \times 1} \times surface\\
=& (\color{#2AA3BB}{4.91-0.126}) + (\color{#ABC8C0}{0.00021+0.00048}) \times surface\\
=& \color{#2AA3BB}{4.784} + \color{#ABC8C0}{0.00069} \times surface
\end{align}
]



---

## Interpretation of interaction effects

- Rate of change in price as the surface area of the painting increases does
vary between paintings by living and non-living artists (different slopes),
- Some paintings by living artists are more expensive than paintings by
non-living artists, and some are not (different intercept).

---
class: middle

# Comparing models

---

## Books weight by volume and cover

```{r book-main-int, echo=FALSE, out.width="55%", fig.asp = 0.8}
book_main_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover, data = allbacks)
book_main_fit_aug <- augment(book_main_fit$fit)

book_int_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(weight ~ volume + cover + volume*cover, data = allbacks)
book_int_fit_aug <- augment(book_int_fit$fit)

p_main <- ggplot() +
  geom_point(book_main_fit_aug, 
             mapping = aes(x = volume, y = weight, color = cover, shape = cover), alpha = 0.7) +
  geom_line(book_main_fit_aug, 
            mapping = aes(x = volume, y = .fitted, color = cover)) +
  labs(title = "Main effects, parallel slopes", 
       subtitle = "weight-hat = volume + cover") +
  scale_color_manual(values = c("#E48957", "#071381"))

p_int <- ggplot() +
  geom_point(book_int_fit_aug, 
             mapping = aes(x = volume, y = weight, color = cover, shape = cover), alpha = 0.7) +
  geom_line(book_int_fit_aug, 
            mapping = aes(x = volume, y = .fitted, color = cover)) +
  labs(title = "Interaction effects, not parallel slopes", 
       subtitle = "weight-hat = volume + cover + volume * cover") +
  scale_color_manual(values = c("#E48957", "#071381"))

p_main /  p_int  + plot_layout(guides = "collect")
```

---

## In pursuit of Occam's razor

- Occam's Razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected.

--
- Model selection follows this principle.

--
- We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model.

--
- In other words, we prefer the simplest best model, i.e. **parsimonious** model.

---

## Comparing models

.pull-left[
```{r ref.label = "book-main-int", echo = FALSE, out.width="100%", fig.asp = 0.8}
```
]
.pull-right[
.question[
Visually, which of the two models is preferable under Occam's razor?
]
]

---

## R-squared

$R^2$ is the percentage of variability in the response variable explained by 
the regression model.

```{r}
glance(book_main_fit)$r.squared
glance(book_int_fit)$r.squared
```

--
- Clearly the model with interactions has a higher $R^2$.

--
- However using $R^2$ for model selection in models with multiple explanatory variables is not a good idea as $R^2$ increases when **any** variable is added to the model.

---

## Adjusted R-squared

... a (more) objective measure for model selection

- Adjusted $R^2$ doesn't increase if the new variable does not provide any new 
informaton or is completely unrelated, as it applies a penalty for number of 
variables included in the model.
- This makes adjusted $R^2$ a preferable metric for model selection in multiple
regression models.

---

## Comparing models with adjusted R-squared

.pull-left[
```{r}
glance(book_main_fit)$r.squared
glance(book_int_fit)$r.squared
```
]
.pull-right[
```{r}
glance(book_main_fit)$adj.r.squared
glance(book_int_fit)$adj.r.squared
```
]

--

.question[
Using adjusted R-squared, which of the two models is preferable?
]

---

## Paintings price by surface and living artist

.pull-left[
```{r echo = FALSE,out.width="100%", fig.asp = 0.8}
p_main_artist /
  p_int_artist  + 
  plot_layout(guides = "collect") & theme(legend.position = "bottom")
```
]
.pull-right[
```{r}
glance(pp_main_fit)$adj.r.squared
glance(pp_int_fit)$adj.r.squared
```
.question[
Which of the two models is preferable?
]
]

--

It appears that adding the interaction actually increased adjusted $R^2$, so we
should indeed use the model with the interactions.

---

## Third order interactions

- Can you? Yes
- Should you? Probably not if you want to interpret these interactions in context
of the data.

---

## Back to log-price vs width and height

- Response variable: `log_price` 
- Explanatory variables: Width and height

.pull-left[
```{r logprice-vs-width2,echo=FALSE, warning=FALSE,out.width="90%"}
ggplot(data = pp, 
       mapping = aes(x = Width_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Log(price) vs. width", 
    x = "Width (inches)", 
    y = "Log(price) (log livres)"
    )
```
]
.pull-right[
```{r logprice-vs-height2,echo=FALSE, warning=FALSE,out.width="90%"}
ggplot(data = pp, 
       mapping = aes(x = Height_in, y = log(price))) +
  geom_point(alpha = 0.5) +
  labs(
    title = "Log(price) vs. height", 
    x = "Height (inches)", 
    y = "Log(price) (log livres)"
    )
```
]

---

## Comparing models with numerical predictors

```{r message=FALSE}
single_reg_fit = linear_reg() %>%
  set_engine("lm") %>%
  fit(log(price) ~ Width_in, data = pp)
glance(single_reg_fit)$adj.r.squared
```

```{r message=FALSE}
mult_reg_fit = linear_reg() %>%
  set_engine("lm") %>%
  fit(log(price) ~ Width_in + Height_in, data = pp)
glance(mult_reg_fit)$adj.r.squared
```

---

## Recap

- The plot of residuals vs predicted/fitted values can be used as diagnostic plot.

--

- Transformations might help, and $\log(y)$ is useful when the response is right skewed and 
for variance stabilization.

  - In this case, the interpretation of the coefficients uses percentage change.

--

- We learnt how to fit and interpret models with multiple predictors

- When working with categorical predictors, main effects mean parallel lines, while interaction effects mean different slopes.

--

- We learnt about Occam's razor and how to use adjusted $R^2$ to do model comparison.
